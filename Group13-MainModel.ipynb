{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy\n",
        "!pip install fonduer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFrDnEKsdTNL",
        "outputId": "17f59840-9828-4a91-d0de-07c451f65e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted<23.8.0,>=18.9.0 (from scrapy)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.7)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.11.17)\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-5.1.1 w3lib-2.1.2 zope.interface-6.1\n",
            "Collecting fonduer\n",
            "  Downloading fonduer-0.9.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fonduer) (4.11.2)\n",
            "Collecting editdistance<0.6.0,>=0.5.2 (from fonduer)\n",
            "  Downloading editdistance-0.5.3.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting snorkel<0.10.0,>=0.9.5 (from fonduer)\n",
            "  Downloading snorkel-0.9.9-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emmental<0.1.0,>=0.0.6 (from fonduer)\n",
            "  Downloading emmental-0.0.9-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml<5.0.0,>=4.2.5 in /usr/local/lib/python3.10/dist-packages (from fonduer) (4.9.3)\n",
            "Collecting mlflow<2.0.0,>=1.1.0 (from fonduer)\n",
            "  Downloading mlflow-1.30.1-py3-none-any.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from fonduer) (1.23.5)\n",
            "Collecting pyyaml<6.0,>=5.1 (from fonduer)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9teUt9AxLiwD"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "class WebSpider ( scrapy . Spider ):\n",
        "  name = 'web_spider'\n",
        "  start_urls = [\n",
        "    'https :// example .com /',\n",
        "  ]\n",
        "\n",
        "  def parse ( self , response ) :\n",
        "    # Check this page has needed data\n",
        "    if response . css ('div.real - estate') . get () is not None :\n",
        "      yield {\n",
        "        'address': response.css('div.real-estate>div.address::text').get(),\n",
        "        'price': response.css('div.real-estate>div.price::text').get(),\n",
        "        'acreage': response.css('div.real-estate>div.acreage::text').get()\n",
        "      }\n",
        "\n",
        "    next_pages = response.css('li.next a::attr(\"href\")').all()\n",
        "    for next_page in next_pages :\n",
        "      yield response.follow(next_page, self.parse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PARALLEL = 4 # assuming a quad - core machine\n",
        "ATTRIBUTE = \"real_estate_fonduer\"\n",
        "conn_string = 'postgresql://localhost:5432/' + ATTRIBUTE\n",
        "\n",
        "from fonduer import Meta , init_logging\n",
        "\n",
        "# Configure logging for Fonduer\n",
        "init_logging(log_dir=\"logs\")\n",
        "\n",
        "session = Meta.init(conn_string).Session()\n"
      ],
      "metadata": {
        "id": "TPtbhvlWdmkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
        "from fonduer.parser import Parser\n",
        "\n",
        "docs_path = 'data/html/'\n",
        "\n",
        "max_docs = 100\n",
        "doc_preprocessor = HTMLDocPreprocessor(docs_path, max_docs=max_docs)\n",
        "\n",
        "corpus_parser = Parser(session, structural=True, lingual=True, language =\"vi\", tabular=True,)\n",
        "% time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)\n"
      ],
      "metadata": {
        "id": "mkj9_3NTd7CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Number = mention_subclass(\"Number\")\n",
        "Address = mention_subclass(\"Address\")\n",
        "Unit = mention_subclass(\"Unit\")\n",
        "Date = mention_subclass(\"Date\")\n",
        "NumberCandidate = candidate_subclass(\"NumberCandidate\", [Number])\n",
        "AddressCandidate = candidate_subclass(\"AddressCandidate\", [Address])\n",
        "PriceCandidate = candidate_subclass(\"PriceCandidate\", [Number, Unit])\n",
        "DateCandidate = candidate_subclass(\"DateCandidate\", [Date])"
      ],
      "metadata": {
        "id": "snqoAktDeeb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mention_number = MentionNgrams(n_max=1)\n",
        "regx_match_number = RegexMatchSpan(rgx='[0 -9]', search=True, full_match=False)\n",
        "\n",
        "mention_unit = MentionNgrams(n_min=1, n_max=3)\n",
        "match_unit = DictionaryMatch(d = price_unit)\n",
        "mention_date = MentionNgrams(n_min=5, n_max=5)\n",
        "match_date = RegexMatchSpan(rgx ='^[0 -3]?[0 -9][\\/ -][0 -3]?[0 -9][\\/ -](?:[0 -9]{2})?[0 -9]{2}$')\n"
      ],
      "metadata": {
        "id": "4UusJb_veXw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_is_title (m: TemporaryContext):\n",
        "  words = m.get_attrib_tokens()\n",
        "  text = '␣'.join(words)\n",
        "  if text.istitle():\n",
        "    false_list = [',', '|', '-']\n",
        "    for char in false_list:\n",
        "      if char in words:\n",
        "        return False\n",
        "    return True\n",
        "  elif text.isdigit():\n",
        "    return True\n",
        "  else :\n",
        "    return False\n",
        "\n",
        "class MentionAddressNgrams (Ngrams) :\n",
        "  def __init__ (\n",
        "      self, n_min: int = 1, n_max: int = 5 ,\n",
        "      split_tokens: Collection[str] = [] ,\n",
        "      address_name: Collection[str] = []\n",
        "    ) -> None:\n",
        "      self.address_name = address_name\n",
        "      Ngrams.__init__(self, n_min = n_min, n_max = n_max, split_tokens = split_tokens)\n",
        "\n",
        "  def apply (self, doc: Document) -> Iterator[TemporarySpanMention]:\n",
        "    for sentence in doc.sentences:\n",
        "      text_sent = sentence.text.lower().replace('_', '␣')\n",
        "      count_address = 0\n",
        "      for address in self.address_name:\n",
        "        if address in text_sent:\n",
        "          count_address += 1\n",
        "        if count_address > 0:\n",
        "          for ts in Ngrams.apply(self, sentence):\n",
        "            yield ts\n",
        "mention_address = MentionAddressNgrams(n_min=1, n_max=3, address_name = list_address_name)\n",
        "match_address = LambdaFunctionMatcher(func = check_is_title)"
      ],
      "metadata": {
        "id": "MaGbIiqQflTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ABSTAIN = -1\n",
        "DUONG = 0\n",
        "PHUONG = 1\n",
        "QUAN = 2\n",
        "TINH = 3\n",
        "DU_AN = 4\n",
        "FALSE_ADDRESS = 5\n",
        "from gmlib.address_tagger.parse import AddressTagger\n",
        "tagger = AddressTagger ()\n",
        "tagger.load_model('accent')\n",
        "\n",
        "def lf_address_extract_model(c):\n",
        "  span_mention = c.address.context\n",
        "  sent_text = span_mention.sentence.text\n",
        "  char_start = span_mention.char_start\n",
        "  char_end = span_mention.char_end\n",
        "  result = tagger.parser(sent_text)\n",
        "  for entity in result ['entities']:\n",
        "    if entity ['start'] == char_start and entity ['end']==char_end:\n",
        "      return map_entity[entity['entity']]\n",
        "  return ABSTAIN\n",
        "\n",
        "def lf_in_dic(c):\n",
        "  text_span = c.des[1].lower().replace('_', '␣')\n",
        "  if text_span in key_address_name['tinh']:\n",
        "    return TINH\n",
        "  if text_span in key_address_name['quan']:\n",
        "    return QUAN\n",
        "  if text_span in key_address_name['phuong']:\n",
        "    return PHUONG\n",
        "  if text_span in key_address_name['duong']:\n",
        "    return DUONG\n",
        "  if text_span in key_address_name['du_an']:\n",
        "    return DU_AN\n",
        "  return ABSTAIN"
      ],
      "metadata": {
        "id": "eZXJvyiti0dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ABSTAIN = -1\n",
        "DIEN_TICH = 0\n",
        "SO_TANG = 1\n",
        "SO_PHONG_NGU = 4\n",
        "SO_PHONG_KHACH = 5\n",
        "SO_PHONG_VE_SINH = 7\n",
        "MAT_TIEN = 8\n",
        "CHIEU_DAI = 9\n",
        "CHIEU_RONG = 10\n",
        "DO_RONG_DUONG = 11\n",
        "SO_DIEN_THOAI = 12\n",
        "NUMBER_FAILE = 13\n",
        "map_entity_label = {\n",
        "  \"ABSTAIN\": ABSTAIN ,\n",
        "  \"surface_size\": DIEN_TICH ,\n",
        "  \"number_of_floors\": SO_TANG ,\n",
        "  \"number_of_rooms\": SO_PHONG_NGU ,\n",
        "  \"number_of_toilets\": SO_PHONG_VE_SINH ,\n",
        "  \"surface_width\": MAT_TIEN ,\n",
        "  \"surface_length\": CHIEU_DAI ,\n",
        "  \"street_width\": DO_RONG_DUONG ,\n",
        "  \"contact_phone\": SO_DIEN_THOAI ,\n",
        "  \"NUMBER_FALSE\": NUMBER_FAILE\n",
        "}\n",
        "\n",
        "from gmlib.content_extract import IE\n",
        "content_ie = IE ()\n",
        "\n",
        "def lf_number_extract_model(c):\n",
        "  span_mention = c.address.context\n",
        "  sent_text = span_mention.sentence.text\n",
        "  char_start = span_mention.char_start\n",
        "  char_end = span_mention.char_end\n",
        "  result = IE.extract(sent_text)\n",
        "  for entity in result ['entities']:\n",
        "    if entity ['start'] == char_start and entity ['end']==char_end :\n",
        "      if entity ['entity'] in map_entity_label :\n",
        "        return map_entity_label[entity['entity']]\n",
        "  return ABSTAIN"
      ],
      "metadata": {
        "id": "UmHyKqX6j4gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGAY_DANG = 0\n",
        "FALSE_DATE = 1\n",
        "date_labels = {\n",
        "  \"submission_date\": NGAY_DANG ,\n",
        "  \"FALSE_DATE\": FALSE_DATE ,\n",
        "}\n",
        "def lf_is_ngay_dang(c):\n",
        "  pre_words = [word.lower() for word in c.des[0]]\n",
        "  pre_text = '␣'.join(pre_words)\n",
        "  pre_text = pre_text.replace ('_', '␣')\n",
        "  if 'đăng' in pre_text :\n",
        "    return NGAY_DANG\n",
        "  if 'cập␣nhật' in pre_text :\n",
        "    return NGAY_DANG\n",
        "  return FALSE_DATE"
      ],
      "metadata": {
        "id": "oCgWO8aHkpCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ABSTAIN = -1\n",
        "GIA = 0\n",
        "FALSE_GIA = 1\n",
        "price_labels = {\n",
        "  \"price\": GIA ,\n",
        "  \"FALSE_GIA\": FALSE_GIA ,\n",
        "}\n",
        "\n",
        "def lf_is_not_false_gia(c):\n",
        "  (number, unit) = c\n",
        "  pre_words = [word.lower() for word in c.des[0]]\n",
        "  pre_text = '␣'.join(pre_words)\n",
        "  lower_unit = unit.context.get_span().lower()\n",
        "  if lower_unit in ['triệu', 'tr']:\n",
        "    if 'lỗ' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if '%' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'chiết_khấu' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'giảm' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'quà' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'tặng' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'voucher' in pre_text :\n",
        "      return FALSE_GIA\n",
        "    if 'trả␣trước' in pre_text :\n",
        "      return FALSE_GIA\n",
        "  if lower_unit in ['triệu/tháng', 'triệu/th', 'tr/th']:\n",
        "    if 'trả_góp' in pre_text :\n",
        "      return FALSE_GIA\n",
        "  return GIA"
      ],
      "metadata": {
        "id": "flqbxZkVll0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ls_address_gold(c):\n",
        "  document = c.document\n",
        "  gold_label_address = document.meta['gold_label']['address']\n",
        "  span_text = c.address.context.get_span().lower()\n",
        "  for key in gold_label_address:\n",
        "    if gold_label_address[key] == span_text:\n",
        "      return map_label [key]\n",
        "  return ABSTAIN\n"
      ],
      "metadata": {
        "id": "cJ3oWG9TmYZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = session.query(Document).order_by(Document.name).all()\n",
        "ld = len(docs)\n",
        "\n",
        "train_docs = set()\n",
        "dev_docs = set()\n",
        "test_docs = set()\n",
        "splits = (0.8, 0.9)\n",
        "data = [(doc.name, doc) for doc in docs]\n",
        "data.sort(key=lambda x: x[0])\n",
        "for i, (doc_name, doc ) in enumerate(data):\n",
        "  if i < splits[0] * ld:\n",
        "    train_docs.add(doc)\n",
        "  elif i < splits[1] * ld:\n",
        "    dev_docs.add(doc)\n",
        "  else :\n",
        "    test_docs.add(doc)"
      ],
      "metadata": {
        "id": "YaOX7bCgmy9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featurization:\n",
        "  textual:\n",
        "    window_feature:\n",
        "      size: 3\n",
        "      combinations: True\n",
        "      isolated: True\n",
        "    word_feature:\n",
        "      window: 7\n",
        "  tabular:\n",
        "    unary_features:\n",
        "      attrib:\n",
        "        - words\n",
        "      get_cell_ngrams:\n",
        "        max: 2\n",
        "      get_head_ngrams:\n",
        "        max: 2\n",
        "      get_row_ngrams:\n",
        "        max: 2\n",
        "      get_col_ngrams:\n",
        "        max: 2\n",
        "learning:\n",
        "  LogisticRegression:\n",
        "    hidden_dim: 100\n",
        "    bias: False"
      ],
      "metadata": {
        "id": "lQSSEejxnWDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
        "  candidate_extractor.apply( docs, split=i, parallelism=PARALLEL)\n",
        "\n",
        "train_cands = candidate_extractor.get_candidates(split = 0)\n",
        "dev_cands = candidate_extractor.get_candidates(split = 1)\n",
        "test_cands = candidate_extractor.get_candidates(split = 2)\n",
        "\n",
        "from fonduer.features import Featurizer\n",
        "\n",
        "featurizer = Featurizer(session, [NumberCandidate, AddressCandidate, PriceCandidate, DateCandidate])\n",
        "\n",
        "% time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
        "% time F_train = featurizer.get_feature_matrices(train_cands)\n",
        "\n",
        "% time featurizer.apply(split=1, parallelism=PARALLEL)\n",
        "% time F_dev=featurizer.get_feature_matrices(dev_cands)\n",
        "\n",
        "% time featurizer.apply(split=2, parallelism = PARALLEL)\n",
        "% time F_test=featurizer.get_feature_matrices(test_cands)"
      ],
      "metadata": {
        "id": "hTCr7IrSoZXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}